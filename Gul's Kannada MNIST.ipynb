{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import time\n",
    "import warnings\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Activation, AveragePooling2D, Input\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, TensorBoard, EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from os.path import join, exists, expanduser\n",
    "from os import listdir, makedirs\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_path = '../input/Kannada-MNIST/'\n",
    "\n",
    "train_path = input_path + 'train.csv'\n",
    "test_path = input_path + 'test.csv'\n",
    "dig_path = input_path + 'Dig-MNIST.csv'\n",
    "sample_path = input_path + 'sample_submission.csv'\n",
    "save_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Load data as Pandas DataFrame\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "dig_data = pd.read_csv(dig_path)\n",
    "sample_submission_data = pd.read_csv(sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing \n",
    "h = w = 28\n",
    "num_train = train_data.shape[0]\n",
    "num_test = test_data.shape[0]\n",
    "num_dig = dig_data.shape[0]\n",
    "input_shape = (h, w, 1)\n",
    "num_classes = 10\n",
    "\n",
    "## Labels\n",
    "y_train = train_data['label']\n",
    "y_dig = dig_data['label']\n",
    "\n",
    "## Images from shape of 784 to (28, 28)\n",
    "X_train = train_data.drop(['label'], axis = 1)\n",
    "X_train = X_train.to_numpy().reshape(num_train, h, w, 1)\n",
    "\n",
    "id_test = test_data['id']\n",
    "X_test = test_data.drop(['id'], axis = 1)\n",
    "X_test = X_test.to_numpy().reshape(num_test, h, w, 1)\n",
    "\n",
    "X_dig = dig_data.drop(['label'], axis = 1)\n",
    "X_dig = X_dig.to_numpy().reshape(num_dig, h, w, 1)\n",
    "\n",
    "# One-hot encoding of labels (instead of 0, 1, 2, 3, etc.)\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "\n",
    "# Normalising inputs\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_dig = X_dig.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "# Create validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "# Create validation data\n",
    "#X_train, X_val1, y_train, y_val1 = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "#X_dig, X_val2, y_dig, y_val2 = train_test_split(X_dig, y_dig, test_size=0.1, random_state=42)\n",
    "#y_val2 = to_categorical(y_val2, num_classes=num_classes)\n",
    "#X_val = np.concatenate((X_val1, X_val2), axis = 0)\n",
    "#y_val = np.concatenate((y_val1, y_val2), axis = 0)\n",
    "\n",
    "print(\"Train data set shape: {}, number of labels: {}\".format(X_train.shape, y_train.shape[1]))\n",
    "print(\"Dig data set shape: {}, number of labels: {}\".format(X_dig.shape, y_dig.shape))\n",
    "print(\"Validation data set shape: {}, number of labels: {}\".format(X_val.shape, y_val.shape))\n",
    "print(\"Test data set shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check couple of images\n",
    "rand_images = X_train[np.random.choice(X_train.shape[0], size=(10,))]\n",
    "\n",
    "fig, ax = plt.subplots(2,5, figsize = (15,5))\n",
    "ax = ax.ravel()\n",
    "print(\"Couple of images to check....\")\n",
    "for i in range(10):\n",
    "    ax[i].imshow(rand_images[i][:,:,0], cmap='gray')\n",
    "    y_train_ = np.argmax(y_train[i])\n",
    "    ax[i].set_title('Class number is: {}'.format(y_train_))\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def __init__(self, val_data):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "    \n",
    "    def on_train_begin(self, logs = {}):\n",
    "        self.val_f1s = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_target = self.validation_data[1]\n",
    "        _val_f1_score = f1_score(val_target, val_predict, average=\"micro\")\n",
    "        self.val_f1s.append(_val_f1_score)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convModel():\n",
    "    \n",
    "    '''\n",
    "    This object is to create a Convolutional Neural Network with a basic architecture for sanity check if a simple model\n",
    "    can make accurate classifications on CIFAR 100 data set. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        \n",
    "        '''\n",
    "        Instantiate the convModel object with arguments below.\n",
    "        '''\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    \n",
    "    # CONV MODEL\n",
    "    def Conv(self, filters, kernel_size, activation = 'relu', input_shape = None):\n",
    "        '''\n",
    "        This method is to create a single Conv layer, which is defined over the order of the layer. When creating a \n",
    "        Conv after feeding the input, then we need to specify the input shape. For other Conv layer, we do not need to\n",
    "        as we are using return_sequences = True, and feed them.\n",
    "\n",
    "        Arguments:\n",
    "        filters -- number of filters, int\n",
    "        kernel_size -- dimension of kernel (filter), int, set to 3 (based on the paper)\n",
    "        activation -- type of activation, set to 'relu', default = linear\n",
    "        input_shape -- image dimensions, (width, height, channels)\n",
    "\n",
    "        Return:\n",
    "        Conv2D layer -- a Keras Conv layer \n",
    "        '''\n",
    "        if input_shape:\n",
    "            return Conv2D(filters = filters,\n",
    "                          kernel_size = kernel_size,\n",
    "                          activation = activation,\n",
    "                          kernel_initializer = 'he_normal',\n",
    "                          bias_initializer= 'glorot_normal',\n",
    "                          padding = 'same',\n",
    "                          input_shape = input_shape)\n",
    "\n",
    "        else:\n",
    "            return Conv2D(filters = filters,\n",
    "                          kernel_size = kernel_size,\n",
    "                          activation = activation,\n",
    "                          kernel_initializer = 'he_normal',\n",
    "                          bias_initializer= 'glorot_normal',\n",
    "                          padding = 'same')\n",
    "\n",
    "    # SANITY CHECK MODEL\n",
    "    def CNN(self):\n",
    "        '''\n",
    "        This method is to create a CNN architecture as follows:\n",
    "        Conv --> MaxPooling --> Dropout --> Conv --> MaxPooling --> Dropout --> Conv --> MaxPooling --> Dropout --> Dense\n",
    "        --> Activation --> Dropout --> Dense --> Activation (prediction)\n",
    "        \n",
    "        Remark: Instead of using Activation layer separately, we applied 'relu' directly when creating Conv layer.\n",
    "        \n",
    "        Arguments:\n",
    "        No arguments needed\n",
    "        \n",
    "        Returns:\n",
    "        model -- a Keras CNN model \n",
    "        '''\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(self.Conv(64, (3,3), input_shape=self.input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(self.Conv(64, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(self.Conv(64, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(self.Conv(128, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(self.Conv(128, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(self.Conv(128, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(self.Conv(256, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(self.Conv(256, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(self.Conv(256, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(self.Conv(512, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(self.Conv(512, (3,3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024,kernel_regularizer=l2(0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1024,kernel_regularizer=l2(0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\n",
    "        model.add(Dropout(0.3))\n",
    "        #model.add(Dense(1024))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Activation('relu'))\n",
    "        #model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Dense(self.num_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def conv_optimizer(self, \n",
    "                       model, \n",
    "                       optimizer):\n",
    "        '''\n",
    "        This method is to optimize the model \n",
    "        '''\n",
    "        \n",
    "        model.compile(optimizer = optimizer , \n",
    "                      loss = \"categorical_crossentropy\", \n",
    "                      metrics=[\"accuracy\"])\n",
    "        \n",
    "        #model.build()\n",
    "  \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "num_epochs = 75\n",
    "batch_size = 50\n",
    "\n",
    "# Optimizers\n",
    "adam_optimizer = Adam(lr = 0.001)\n",
    "sgd_optimizer = SGD(lr=0.001, decay=0, momentum=0.9, nesterov=True)\n",
    "rmsprop_optimizer = RMSprop(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model \n",
    "cnn_model = convModel(input_shape, num_classes = num_classes)\n",
    "cnn_network = cnn_model.CNN()\n",
    "\n",
    "# Build model\n",
    "cnn_built_model = cnn_model.conv_optimizer(model = cnn_network, \n",
    "                                           optimizer = adam_optimizer)\n",
    "\n",
    "# CALLBACKS \n",
    "## Checkpoint Callback for the process\n",
    "model_name = 'cnn_model_checkpoints.h5'\n",
    "filepath = os.path.join(save_path, model_name)\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_accuracy',\n",
    "                             mode='max', \n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor = 'val_loss',\n",
    "                              mode='min', \n",
    "                              verbose=1,\n",
    "                              patience=50,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "learning_rate_reducer = ReduceLROnPlateau(monitor = 'val_accuracy',\n",
    "                                          factor=0.75,\n",
    "                                          cooldown=0,\n",
    "                                          patience=3,\n",
    "                                          verbose=1,\n",
    "                                          mode='max',\n",
    "                                          min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, learning_rate_reducer, earlystopping]\n",
    "\n",
    "# Image augmentation on training data set\n",
    "train_datagen = ImageDataGenerator(rescale=1.0,\n",
    "                                   rotation_range=10,\n",
    "                                   width_shift_range=0.25,\n",
    "                                   height_shift_range=0.25,\n",
    "                                   shear_range=0.1,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=False)\n",
    "\n",
    "# Image augmentation on validation data set\n",
    "val_datagen = ImageDataGenerator(rescale=1.0)\n",
    "\n",
    "# Image generators\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size)\n",
    "\n",
    "\n",
    "#\n",
    "history = cnn_built_model.fit_generator(train_generator, \n",
    "                                        epochs = num_epochs,\n",
    "                                        validation_data = val_generator,\n",
    "                                        steps_per_epoch = X_train.shape[0]//batch_size,\n",
    "                                        validation_steps = X_val.shape[0]//batch_size,\n",
    "                                        verbose = 1,\n",
    "                                        callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "cnn_trained_network = load_model('cnn_model_checkpoints.h5')\n",
    "\n",
    "# Check predictions on Dig_MNIST data set\n",
    "y_dig_pred = cnn_trained_network.predict(X_dig)\n",
    "y_dig_pred = np.argmax(y_dig_pred, axis = 1)\n",
    "\n",
    "dig_acc = accuracy_score(y_dig_pred, y_dig)\n",
    "print(\"Dig-MNIST data set accuracy is: {}\".format(dig_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data set predictions and save\n",
    "y_pred = cnn_trained_network.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "sample_submission_data['label'] = y_pred\n",
    "sample_submission_data.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
